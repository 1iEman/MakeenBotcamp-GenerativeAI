{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Day 4: Document Summarization with Transformers\n",
        "\n",
        "Welcome to Day 4 of the Generative AI workshop!\n",
        "\n",
        "Today we will focus on **text summarization** using open-source transformer models. Summarization is a core capability of modern LLMs that allows us to extract concise, informative summaries from longer documents.\n",
        "\n",
        "We'll work through a complete pipeline:\n",
        "- Upload a PDF file\n",
        "- Extract and chunk the text\n",
        "- Load a summarization model\n",
        "- Run summarization at the chunk level and for the full document\n",
        "\n",
        "By the end of this lab, you‚Äôll understand how summarization works and how to apply it to real-world documents.\n"
      ],
      "metadata": {
        "id": "cSOQTLzKsBLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 1: Install Required Packages\n",
        "\n",
        "We'll install the following libraries:\n",
        "- `PyPDF2` to extract text from PDFs\n",
        "- `transformers` to load our summarization model\n",
        "- `sentence-transformers` to optionally embed for semantic filtering\n"
      ],
      "metadata": {
        "id": "1xuGb8XysFcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 transformers sentence-transformers rouge-score tqdm nltk --quiet\n"
      ],
      "metadata": {
        "id": "fD4AoyJZsCEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08065517-f5a5-4d80-b010-0b9f0e1020fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 2: Import Required Libraries\n"
      ],
      "metadata": {
        "id": "ZRj1V3_WsWI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "3bM39y9SsHSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Step 3: Upload and Extract Text from PDF\n",
        "\n",
        "In this step, we'll learn how to upload a PDF file and extract its text content using the **PyPDF2** library. This is a crucial skill for processing document-based data in machine learning and NLP projects.\n",
        "\n",
        "## What We'll Learn:\n",
        "- How to upload files in Google Colab\n",
        "- Extract text from PDF documents\n",
        "- Handle multi-page PDFs efficiently\n",
        "- Process and clean extracted text\n",
        "\n",
        "## Why This Matters:\n",
        "PDF text extraction is essential for:\n",
        "- Document analysis and summarization\n",
        "- Information retrieval systems\n",
        "- Training data preparation for NLP models\n",
        "- Automated document processing pipelines\n",
        "\n",
        "## Your Task:\n",
        "Complete the code below by filling in the blanks. Pay attention to the hints provided in the comments!"
      ],
      "metadata": {
        "id": "jMIWxtWgs8bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import files\n",
        "from PyPDF2 import PdfReader  # Hint: PyPDF2, PdfReader (in that order)\n",
        "\n",
        "# Step 1: Upload PDF file\n",
        "print(\"üìÅ Please select your PDF file to upload...\")\n",
        "uploaded = files.upload()  # Hint: Choose one -> upload() | download() | read()\n",
        "\n",
        "# Step 2: Get the filename and create PDF reader\n",
        "filename = next(iter(uploaded))  # Hint: The variable that contains uploaded files -> uploaded | files | reader\n",
        "reader = PdfReader(filename)     # Hint: The class we imported -> PdfReader | FileReader | TextReader\n",
        "\n",
        "print(f\"\\nüìä PDF Analysis:\")\n",
        "print(f\"üìÑ Number of pages: {len(reader.pages)}\")  # Hint: PDF attribute -> pages | text | content\n",
        "\n",
        "# Step 3: Extract all text from PDF\n",
        "# Hint: Fill in the blanks to extract text from each page\n",
        "# Pattern: [PAGE.extract_text() for PAGE in reader.PAGES if page.extract_text()]\n",
        "full_text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "#                       ^page                    ^page              ^pages\n",
        "\n",
        "# Step 4: Display results\n",
        "print(f\"\\n‚úÖ PDF uploaded and processed!\")\n",
        "print(f\"üìÇ Filename: {filename}\")  # Hint: Variable storing the file name -> filename | uploaded | reader\n",
        "print(f\"üìè Total text length: {len(full_text)} characters\")  # Hint: Variable with all text -> full_text | text | content\n",
        "\n",
        "# Bonus: Display a preview of the text\n",
        "print(f\"üìù First 200 characters preview:\")\n",
        "print(\"-\" * 50)\n",
        "# Hint: All three blanks should be the same variable containing our extracted text\n",
        "print(full_text[:200] + \"...\" if len(full_text) > 200 else full_text)\n",
        "#      ^full_text                    ^full_text           ^full_text\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - This step reads each page of the PDF using PyPDF2\n",
        "# - We join the extracted text into a single string with newlines\n",
        "# - The resulting full_text variable will be used for chunking and embedding in the next steps\n",
        "# - We filter out empty pages to avoid processing blank content\n",
        "# - List comprehension makes the code more efficient and readable\n"
      ],
      "metadata": {
        "id": "Y8OQ8Q8_sYnQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "02de05c2-c480-43db-851b-b67d2f644526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Please select your PDF file to upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b75072c5-2ab5-4d87-b452-5edda0577d44\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b75072c5-2ab5-4d87-b452-5edda0577d44\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving The-difference-between-reliability-and-agreement_2.pdf to The-difference-between-reliability-and-agreement_2.pdf\n",
            "\n",
            "üìä PDF Analysis:\n",
            "üìÑ Number of pages: 2\n",
            "\n",
            "‚úÖ PDF uploaded and processed!\n",
            "üìÇ Filename: The-difference-between-reliability-and-agreement_2.pdf\n",
            "üìè Total text length: 9212 characters\n",
            "üìù First 200 characters preview:\n",
            "--------------------------------------------------\n",
            "LETTERS TO THE EDITOR\n",
            "The difference between reliability and agreement\n",
            "To the Editor:\n",
            "In their article, Costa-Santos et al. [1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üëÅÔ∏è Step 3.1: Optional - Preview Extracted Text\n",
        "\n",
        "Now that we've successfully extracted text from our PDF, let's take a look at what we've got! This optional step helps us verify that our text extraction worked properly and gives us a sense of the content we'll be working with.\n",
        "\n",
        "## Why Preview the Text?\n",
        "- **Quality Check**: Ensure the extraction captured readable text\n",
        "- **Content Understanding**: Get familiar with the document structure\n",
        "- **Debugging**: Identify any formatting issues early\n",
        "- **Data Validation**: Confirm we have meaningful content to work with\n",
        "\n",
        "## Your Task:\n",
        "Complete the code to preview and analyze your extracted text. Use the hints to fill in the blanks!\n",
        "\n",
        "*Note: We're showing the first 1000 characters to get a good preview without overwhelming the output.*"
      ],
      "metadata": {
        "id": "uNNGHwvtuQIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Preview the extracted text to verify successful extraction\n",
        "print(\"üìñ EXTRACTED TEXT PREVIEW (First 1000 characters):\")\n",
        "print(\"=\" * 60)\n",
        "print(full_text[:1000])\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# üìä Additional text statistics\n",
        "print(f\"\\nüìà TEXT STATISTICS:\")\n",
        "print(f\"üìè Total characters: {len(full_text):,}\")\n",
        "print(f\"üìù Total words: {len(full_text.split()):,}\")\n",
        "print(f\"üìÑ Total lines: {len(full_text.split(chr(10))):,}\")\n",
        "\n",
        "# üéØ Quick content check\n",
        "if len(full_text) > 100:\n",
        "    print(f\"‚úÖ Text extraction successful - Ready for processing!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Text seems too short. Check your PDF file.\")"
      ],
      "metadata": {
        "id": "D111C1KvtnJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b35b1d-f7c5-4618-c6a7-3e22c8c071ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ EXTRACTED TEXT PREVIEW (First 1000 characters):\n",
            "============================================================\n",
            "LETTERS TO THE EDITOR\n",
            "The difference between reliability and agreement\n",
            "To the Editor:\n",
            "In their article, Costa-Santos et al. [1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpreting\n",
            "reliability and agreement coefÔ¨Åcients arising from the same\n",
            "measurement situation. Debates and proposals about what\n",
            "the correct coefÔ¨Åcients to measure agreement and reliabilityare can be traced back to the early 1980s [2,3]. Various ap-\n",
            "proaches were discussed to overcome the ‚Äò‚Äòlimitations‚Äô‚Äô and‚Äò‚Äòdrawbacks‚Äô‚Äô of reliability measures (e.g., Refs. [4,5]), and\n",
            "even today, new alternatives are proposed (e.g., Refs. [6,7]).\n",
            "However, it seems that much of the confusion around re-\n",
            "liability and agreement estimation was and is caused by\n",
            "conceptual ambiguities. There are important differences\n",
            "between the concepts of agreement and reliability (e.g.,\n",
            "Refs. [8,9] ).Agreement points to the question, whether di-\n",
            "agnoses, scores, or judgments are identical or similar or thedegree to which they diffe\n",
            "============================================================\n",
            "\n",
            "üìà TEXT STATISTICS:\n",
            "üìè Total characters: 9,212\n",
            "üìù Total words: 1,296\n",
            "üìÑ Total lines: 124\n",
            "‚úÖ Text extraction successful - Ready for processing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÇÔ∏è Step 4: Chunk the Text\n",
        "\n",
        "Large documents can be overwhelming for AI models and search systems. To make our text more manageable and improve processing efficiency, we'll break it into smaller, meaningful chunks.\n",
        "\n",
        "## Why Do We Need Text Chunking?\n",
        "\n",
        "### üéØ **Performance Benefits:**\n",
        "- **Memory Efficiency**: Smaller chunks use less computational resources\n",
        "- **Better Embeddings**: Models work better with focused, coherent text segments\n",
        "- **Improved Search**: Users can find specific information more easily\n",
        "- **Parallel Processing**: Multiple chunks can be processed simultaneously\n",
        "\n",
        "### üìè **Chunking Strategy:**\n",
        "- **Sentence-Based**: We split at sentence boundaries to maintain meaning\n",
        "- **Size Control**: Each chunk stays under 2000 characters (default)\n",
        "- **Context Preservation**: We don't break sentences in the middle\n",
        "\n",
        "## Your Task:\n",
        "Complete the chunking function by filling in the blanks. Pay attention to the algorithm steps in the comments!"
      ],
      "metadata": {
        "id": "xbxU_K1zuSp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Hint: We need the 're' module for regex operations\n",
        "\n",
        "def chunk_text(text, max_length=2000):\n",
        "    \"\"\"\n",
        "    Split text into manageable chunks while preserving sentence boundaries.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked\n",
        "        max_length (int): Maximum characters per chunk (default: 2000)\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks\n",
        "    \"\"\"\n",
        "    # Step 1: Split text into sentences using regex\n",
        "    # Hint: Use re.split() with the pattern r'(?<=[.!?])\\s+'\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    #            ^re                           ^text\n",
        "\n",
        "    # Step 2: Initialize variables for chunking\n",
        "    # Hint: We need an empty list for chunks and an empty string for current chunk\n",
        "    chunks, chunk = [], \"\"\n",
        "    #                ^[]    ^\"\"\n",
        "\n",
        "    # Step 3: Build chunks by combining sentences\n",
        "    for sentence in sentences:  # Hint: Loop through our sentences list\n",
        "        # Check if adding this sentence would exceed our limit\n",
        "        if len(chunk) + len(sentence) <= max_length:  # Hint: chunk + sentence\n",
        "            chunk += sentence + \" \"  # Hint: Add the sentence with a space\n",
        "        else:\n",
        "            # Current chunk is full, save it and start new one\n",
        "            if chunk:  # Only append if chunk has content\n",
        "                chunks.append(chunk.strip())  # Hint: Add chunk to list -> append() | add() | insert()\n",
        "            chunk = sentence + \" \"  # Hint: Start new chunk with current sentence\n",
        "\n",
        "    # Step 4: Don't forget the last chunk!\n",
        "    if chunk:  # Hint: Check if chunk has content\n",
        "        chunks.append(chunk.strip())  # Hint: Add the final chunk\n",
        "\n",
        "    return chunks  # Hint: Return our list of chunks\n",
        "\n",
        "# Apply chunking to our extracted text\n",
        "print(\"üîÑ Chunking the extracted text...\")\n",
        "chunks = chunk_text(full_text)  # Hint: Pass our extracted text -> full_text | text | content\n",
        "\n",
        "# Display results\n",
        "print(f\"‚úÖ Text successfully chunked!\")\n",
        "print(f\"üìä Created {len(chunks)} chunks\")  # Hint: Count our chunks list\n",
        "print(f\"üìè Average chunk size: {sum(len(chunk) for chunk in chunks) // len(chunks)} characters\")\n",
        "#                                                            ^chunks        ^chunks\n",
        "\n",
        "# Preview first chunk\n",
        "print(f\"\\nüìñ Preview of first chunk:\")\n",
        "print(\"-\" * 50)\n",
        "# Hint: Show first 300 characters of the first chunk, add \"...\" if longer\n",
        "print(chunks[0] + \"...\" if len(chunks[0]) > 300 else chunks[0])\n",
        "#      ^chunks                       ^chunks              ^chunks\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - Regular expressions (regex) help us split text at sentence boundaries\n",
        "# - The strip() method removes leading/trailing whitespace\n",
        "# - We use len() to check if adding a sentence would exceed our character limit\n",
        "# - List comprehension with sum() calculates the average chunk size efficiently\n",
        "# - Always handle the last chunk separately since the loop might not catch it\n"
      ],
      "metadata": {
        "id": "p30tG23duRsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7b556a-44e0-4b38-c73b-573ccc60cd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Chunking the extracted text...\n",
            "‚úÖ Text successfully chunked!\n",
            "üìä Created 5 chunks\n",
            "üìè Average chunk size: 1841 characters\n",
            "\n",
            "üìñ Preview of first chunk:\n",
            "--------------------------------------------------\n",
            "LETTERS TO THE EDITOR\n",
            "The difference between reliability and agreement\n",
            "To the Editor:\n",
            "In their article, Costa-Santos et al. [1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpreting\n",
            "reliability and agreement coefÔ¨Åcients arising from the same\n",
            "measurement situation. Debates and proposals about what\n",
            "the correct coefÔ¨Åcients to measure agreement and reliabilityare can be traced back to the early 1980s [2,3]. Various ap-\n",
            "proaches were discussed to overcome the ‚Äò‚Äòlimitations‚Äô‚Äô and‚Äò‚Äòdrawbacks‚Äô‚Äô of reliability measures (e.g., Refs. [4,5]), and\n",
            "even today, new alternatives are proposed (e.g., Refs. [6,7]). However, it seems that much of the confusion around re-\n",
            "liability and agreement estimation was and is caused by\n",
            "conceptual ambiguities. There are important differences\n",
            "between the concepts of agreement and reliability (e.g.,\n",
            "Refs. [8,9] ).Agreement points to the question, whether di-\n",
            "agnoses, scores, or judgments are identical or similar or thedegree to which they differ. In this situation, the absolutedegree of measurement error is of interest. Consequently,any variability between subjects or the distribution of therated trait in the population does not matter. For instance,\n",
            "percent agreement for nominal data or limits of agreement\n",
            "for interval and ratio data are excellent measures becausethey provide this very kind of information in a simple man-ner. On the other hand, there are the reliability coefÔ¨Åcients. Reliability is typically deÔ¨Åned as the ratio of variability be-tween scores of the same subjects (e.g., by different ratersor at different times) to the total variability of all scoresin the sample. Therefore, reliability coefÔ¨Åcients (e.g.,\n",
            "kappa, intraclass correlation coefÔ¨Åcient) provide informa-\n",
            "tion about the ability of the scores to distinguish betweensubjects. From this, it also follows that reliability coefÔ¨Å-cients must be low when there is little variability amongthe scores or diagnoses obtained from the instrument underinvestigation....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Step 4.1: Optional - Inspect Chunks\n",
        "\n",
        "Now that we've chunked our text, let's examine what we've created! This inspection step helps us understand the quality and distribution of our chunks, ensuring they're suitable for downstream processing.\n",
        "\n",
        "## What We'll Analyze:\n",
        "- **üìä Statistical Overview**: Count, averages, min/max sizes\n",
        "- **üìñ Content Preview**: Look at actual chunk content\n",
        "- **üìà Distribution Analysis**: Visualize chunk size patterns\n",
        "- **üéØ Quality Assessment**: Ensure chunks are meaningful and well-sized\n",
        "\n",
        "## Your Task:\n",
        "Complete the chunk analysis code by filling in the blanks. Use the hints to calculate statistics and create visualizations!"
      ],
      "metadata": {
        "id": "1z9xiiDGuZMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced chunk inspection with comprehensive statistics\n",
        "import statistics\n",
        "\n",
        "# üìä Calculate comprehensive chunk statistics\n",
        "total_chunks = len(chunks)\n",
        "chunk_lengths = [len(chunk) for chunk in chunks]\n",
        "avg_chunk_length = statistics.mean(chunk_lengths)\n",
        "min_chunk_length = min(chunk_lengths)\n",
        "max_chunk_length = max(chunk_lengths)\n",
        "median_chunk_length = statistics.median(chunk_lengths)\n",
        "\n",
        "# üìà Display detailed statistics\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä CHUNK STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìù Total number of chunks: {total_chunks}\")\n",
        "print(f\"üìè Average chunk length: {avg_chunk_length:.1f} characters\")\n",
        "print(f\"üìâ Minimum chunk length: {min_chunk_length} characters\")\n",
        "print(f\"üìà Maximum chunk length: {max_chunk_length} characters\")\n",
        "print(f\"üìä Median chunk length: {median_chunk_length:.1f} characters\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# üìñ Display sample chunks for content review\n",
        "print(\"üìñ SAMPLE CHUNKS:\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(min(3, len(chunks))):\n",
        "    print(f\"--- Chunk {i+1} ({len(chunks[i])} chars) ---\")\n",
        "    print(chunks[i][:200] + \"...\" if len(chunks[i]) > 200 else chunks[i])\n",
        "    print()\n",
        "\n",
        "# üìà Visual chunk length distribution\n",
        "print(\"üìà CHUNK LENGTH DISTRIBUTION (First 10 chunks):\")\n",
        "print(\"-\" * 50)\n",
        "for i, length in enumerate(chunk_lengths[:10]):\n",
        "    # Create a simple bar chart using characters\n",
        "    bar_length = int(length / max_chunk_length * 30)  # Scale to 30 chars max\n",
        "    bar = \"‚ñà\" * bar_length\n",
        "    print(f\"Chunk {i+1:2d}: {length:4d} chars |{bar}\")\n",
        "\n",
        "if len(chunk_lengths) > 10:\n",
        "    print(f\"... and {len(chunk_lengths) - 10} more chunks\")\n",
        "\n",
        "print()\n",
        "\n",
        "# üéØ Quick summary for easy reference\n",
        "print(\"üîç QUICK SUMMARY:\")\n",
        "print(f\"   üìö {total_chunks} chunks | üìè Avg: {avg_chunk_length:.0f} chars | üìä Range: {min_chunk_length}-{max_chunk_length} chars\")\n",
        "\n",
        "# ‚úÖ Quality assessment\n",
        "print(f\"\\n‚úÖ QUALITY CHECK:\")\n",
        "if avg_chunk_length > 100 and avg_chunk_length < 2500:\n",
        "    print(\"‚úÖ Chunk sizes look good for processing!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Consider adjusting chunk size parameters.\")\n",
        "\n",
        "if max_chunk_length <= 2000:\n",
        "    print(\"‚úÖ All chunks within size limit!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Some chunks exceed 2000 characters (max: {max_chunk_length})\")"
      ],
      "metadata": {
        "id": "M8b3LkKSuVb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2317ac6-8ea8-44ba-f3de-e116eb340575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üìä CHUNK STATISTICS\n",
            "============================================================\n",
            "üìù Total number of chunks: 5\n",
            "üìè Average chunk length: 1841.6 characters\n",
            "üìâ Minimum chunk length: 1697 characters\n",
            "üìà Maximum chunk length: 1997 characters\n",
            "üìä Median chunk length: 1821.0 characters\n",
            "============================================================\n",
            "\n",
            "üìñ SAMPLE CHUNKS:\n",
            "----------------------------------------\n",
            "--- Chunk 1 (1997 chars) ---\n",
            "LETTERS TO THE EDITOR\n",
            "The difference between reliability and agreement\n",
            "To the Editor:\n",
            "In their article, Costa-Santos et al. [1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpr...\n",
            "\n",
            "--- Chunk 2 (1907 chars) ---\n",
            "This occurs when the range of obtainedscores is restricted or prevalence is very high or very low.For example, if all raters rate medical students as ‚Äò‚Äòexcel-\n",
            "lent,‚Äô‚Äô the agreement is perfect, but the...\n",
            "\n",
            "--- Chunk 3 (1697 chars) ---\n",
            "Streiner\n",
            "Department of Psychiatry\n",
            "University of Toronto, Toronto\n",
            "Ontario, Canada\n",
            "References\n",
            "[1] Costa-Santos C, Bernardes J, Ayres-de-Campos D, Costa A, Costa C. The limits of agreement and the intrac...\n",
            "\n",
            "üìà CHUNK LENGTH DISTRIBUTION (First 10 chunks):\n",
            "--------------------------------------------------\n",
            "Chunk  1: 1997 chars |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Chunk  2: 1907 chars |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Chunk  3: 1697 chars |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Chunk  4: 1786 chars |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Chunk  5: 1821 chars |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "üîç QUICK SUMMARY:\n",
            "   üìö 5 chunks | üìè Avg: 1842 chars | üìä Range: 1697-1997 chars\n",
            "\n",
            "‚úÖ QUALITY CHECK:\n",
            "‚úÖ Chunk sizes look good for processing!\n",
            "‚úÖ All chunks within size limit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Step 5: Load Summarization Model\n",
        "\n",
        "Now we'll load a powerful AI model to summarize our text chunks. We're using **Google's FLAN-T5-Base** - a state-of-the-art language model that's been fine-tuned for instruction-following tasks like summarization.\n",
        "\n",
        "## About FLAN-T5-Base üß†\n",
        "\n",
        "### **What is FLAN-T5?**\n",
        "- **FLAN**: Fine-tuned Language Net - Google's instruction-tuned model family\n",
        "- **T5**: Text-to-Text Transfer Transformer - treats all NLP tasks as text generation\n",
        "- **Base Size**: Balanced model with ~250M parameters (good performance vs. speed)\n",
        "\n",
        "### **Why This Model?**\n",
        "- ‚úÖ **Excellent Summarization**: Specifically trained for text summarization tasks\n",
        "- ‚úÖ **Instruction Following**: Understands natural language prompts\n",
        "- ‚úÖ **Efficient Size**: Fast enough for real-time processing\n",
        "- ‚úÖ **Open Source**: Free to use and well-documented\n",
        "\n",
        "## Your Task:\n",
        "Complete the model loading code by filling in the blanks. Pay attention to the two main components we need!"
      ],
      "metadata": {
        "id": "HgUj05QJuivN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for model loading\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Hint: AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available for faster processing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "# Define the model name - Google's FLAN-T5-Base\n",
        "model_name = \"google/flan-t5-base\"  # Hint: \"google/flan-t5-base\" | \"openai/gpt-3\" | \"facebook/bart-base\"\n",
        "print(f\"ü§ñ Loading model: {model_name}\")\n",
        "\n",
        "# Step 1: Load the tokenizer\n",
        "print(\"üìù Loading tokenizer...\")\n",
        "# Hint: Use AutoTokenizer.from_pretrained() with the model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"‚úÖ Tokenizer loaded successfully!\")\n",
        "\n",
        "# Step 2: Load the model\n",
        "print(\"üß† Loading model (this may take a moment)...\")\n",
        "# Hint: Use AutoModelForSeq2SeqLM.from_pretrained() with the model name\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Step 3: Move model to appropriate device (GPU if available)\n",
        "model = model.to(device)  # Hint: Move to our device variable\n",
        "print(f\"üìç Model moved to {device}\")\n",
        "\n",
        "# Display model information\n",
        "print(f\"\\nüìä MODEL INFORMATION:\")\n",
        "print(f\"üìù Tokenizer vocabulary size: {tokenizer.vocab_size:,}\")  # Hint: tokenizer | model | device\n",
        "print(f\"üß† Model parameters: ~250M\")\n",
        "print(f\"üìè Max input length: {tokenizer.model_max_length:,} tokens\")  # Hint: tokenizer | model | device\n",
        "\n",
        "# Test tokenizer with a sample\n",
        "sample_text = \"This is a test sentence to verify our tokenizer is working correctly.\"\n",
        "# Hint: Use tokenizer.encode() to convert text to tokens\n",
        "tokens = tokenizer.encode(sample_text)\n",
        "print(f\"\\nüîç TOKENIZER TEST:\")\n",
        "print(f\"Input: {sample_text}\")\n",
        "print(f\"Tokens: {len(tokens)} tokens\")  # Hint: Count the tokens we just created\n",
        "print(f\"‚úÖ Tokenizer working correctly!\")\n",
        "\n",
        "print(f\"\\nüéâ Summarization model ready for use!\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - AutoTokenizer converts text into numerical tokens that models can understand\n",
        "# - AutoModelForSeq2SeqLM is designed for sequence-to-sequence tasks like summarization\n",
        "# - from_pretrained() downloads and loads pre-trained models from Hugging Face\n",
        "# - Moving models to GPU (if available) significantly speeds up processing\n",
        "# - Testing the tokenizer ensures everything loaded correctly before proceeding\n"
      ],
      "metadata": {
        "id": "PvJ4OBksugIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd4dda3-5e5d-4df8-e8a4-c2c87632c3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è  Using device: cuda\n",
            "ü§ñ Loading model: google/flan-t5-base\n",
            "üìù Loading tokenizer...\n",
            "‚úÖ Tokenizer loaded successfully!\n",
            "üß† Loading model (this may take a moment)...\n",
            "‚úÖ Model loaded successfully!\n",
            "üìç Model moved to cuda\n",
            "\n",
            "üìä MODEL INFORMATION:\n",
            "üìù Tokenizer vocabulary size: 32,100\n",
            "üß† Model parameters: ~250M\n",
            "üìè Max input length: 512 tokens\n",
            "\n",
            "üîç TOKENIZER TEST:\n",
            "Input: This is a test sentence to verify our tokenizer is working correctly.\n",
            "Tokens: 16 tokens\n",
            "‚úÖ Tokenizer working correctly!\n",
            "\n",
            "üéâ Summarization model ready for use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Step 6: Define a Summarization Function\n",
        "\n",
        "Now we'll create a function that takes a text chunk and returns a concise summary. This function will be the core of our document summarization system!\n",
        "\n",
        "## How Our Summarization Works üîÑ\n",
        "\n",
        "### **The Process:**\n",
        "1. **üìù Prompt Creation**: We give the model clear instructions\n",
        "2. **üî¢ Tokenization**: Convert text to numbers the model understands  \n",
        "3. **üß† Generation**: Model creates a summary based on our prompt\n",
        "4. **üìñ Decoding**: Convert model output back to readable text\n",
        "\n",
        "### **Key Parameters Explained:**\n",
        "\n",
        "#### **üéØ Generation Settings:**\n",
        "- **`max_new_tokens=100`**: Limits summary to ~100 tokens (roughly 75-80 words)\n",
        "- **`do_sample=True`**: Enables creative, varied outputs (vs. always picking most likely words)\n",
        "- **`temperature=0.9`**: Controls creativity (0.0=boring, 1.0=creative, 2.0=chaotic)\n",
        "- **`repetition_penalty=1.1`**: Prevents the model from repeating phrases\n",
        "\n",
        "#### **üîß Technical Settings:**\n",
        "- **`return_tensors=\"pt\"`**: Returns PyTorch tensors (compatible with our model)\n",
        "- **`truncation=True`**: Cuts off text if it's too long for the model\n",
        "- **`padding=True`**: Ensures consistent input sizes for batch processing\n",
        "- **`skip_special_tokens=True`**: Removes technical tokens from final output\n",
        "\n",
        "### **Why This Approach?**\n",
        "- ‚úÖ **Clear Instructions**: The prompt tells the model exactly what we want\n",
        "- ‚úÖ **Consistent Quality**: Same prompt ensures similar summary styles\n",
        "- ‚úÖ **Optimal Length**: 3 sentences provide good detail without being too long\n",
        "- ‚úÖ **Flexible Output**: Temperature allows for natural, varied summaries\n",
        "\n",
        "## Your Task:\n",
        "Complete the summarization function by filling in the blanks. Pay attention to the process steps!"
      ],
      "metadata": {
        "id": "06utH9_Wut1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "    \"\"\"\n",
        "    Generate a concise summary of the input text using FLAN-T5.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text chunk to summarize\n",
        "\n",
        "    Returns:\n",
        "        str: A 3-sentence summary of the input text\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create a clear prompt for the model\n",
        "    # Hint: Tell the model to \"Summarize this concisely in 3 sentences:\" followed by the text\n",
        "    prompt = f\"Summarize this concisely in 3 sentences:\\n{text}\"\n",
        "    #         \"Summarize this concisely in 3 sentences\"          text\n",
        "\n",
        "    # Step 2: Tokenize the prompt\n",
        "    # Hint: Use our tokenizer with return_tensors=\"pt\", truncation=True, padding=True\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",     # Hint: \"pt\" | \"tf\" | \"np\"\n",
        "        truncation=True,         # Hint: True | False\n",
        "        padding=True,            # Hint: True | False\n",
        "        max_length=512\n",
        "    ).to(device)  # Hint: Move to our device variable\n",
        "\n",
        "    # Step 3: Generate summary using the model\n",
        "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "        outputs = model.generate(  # Hint: Use our loaded model\n",
        "            **inputs,                    # Pass our tokenized input\n",
        "            max_new_tokens=100,         # Hint: Limit to ~100 tokens\n",
        "            do_sample=False,            # Hint: True for variety | False for consistency\n",
        "            temperature=0.9,            # Hint: 0.9 for balanced creativity\n",
        "            repetition_penalty=1.1,     # Hint: 1.1 to reduce repetition\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Step 4: Decode the generated tokens back to text\n",
        "    # Hint: Use tokenizer.decode() with skip_special_tokens=True\n",
        "    summary = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True    # Hint: True | False - remove technical tokens?\n",
        "    )\n",
        "\n",
        "    # Step 5: Clean up the output\n",
        "    # Remove the original prompt from the generated text if it appears\n",
        "    if \"Summarize this concisely\" in summary:\n",
        "        summary = summary.split(\"Summarize this concisely in 3 sentences:\")[-1].strip()\n",
        "\n",
        "    return summary  # Hint: Return our cleaned summary\n",
        "\n",
        "# Test the function with a sample chunk\n",
        "print(\"üß™ Testing summarization function...\")\n",
        "if len(chunks) > 0:  # Hint: Check if we have chunks available\n",
        "    sample_chunk = chunks[0]  # Hint: Get the first chunk\n",
        "    print(f\"\\nüìñ Original text ({len(sample_chunk)} chars):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(sample_chunk + \"...\" if len(sample_chunk) > 300 else sample_chunk)\n",
        "\n",
        "    print(f\"\\nüìù Generated Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    # Hint: Call our summarize function with the sample chunk\n",
        "    sample_summary = summarize(sample_chunk)\n",
        "    print(sample_summary)\n",
        "\n",
        "    print(f\"\\nüìä Compression Stats:\")\n",
        "    print(f\"üìè Original: {len(sample_chunk)} characters\")  # Hint: sample_chunk | summary | text\n",
        "    print(f\"üìè Summary: {len(sample_summary)} characters\")   # Hint: sample_summary | chunk | output\n",
        "    # Hint: Calculate compression ratio by dividing original length by summary length\n",
        "    print(f\"üìà Compression ratio: {len(sample_chunk)/len(sample_summary):.1f}:1\")\n",
        "    print(f\"‚úÖ Summarization function working correctly!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No chunks available for testing\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - Function definitions use def keyword followed by function name and parameters\n",
        "# - f-strings allow us to embed variables directly in text using {variable}\n",
        "# - torch.no_grad() context manager improves performance during inference\n",
        "# - The model.generate() method is where the AI magic happens\n",
        "# - String cleaning ensures our output is properly formatted\n",
        "# - Testing functions with sample data is crucial before processing large datasets\n"
      ],
      "metadata": {
        "id": "hoMcf5U8ulKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd15ef62-a009-46bf-f6aa-334e0531ef07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing summarization function...\n",
            "\n",
            "üìñ Original text (1997 chars):\n",
            "--------------------------------------------------\n",
            "LETTERS TO THE EDITOR\n",
            "The difference between reliability and agreement\n",
            "To the Editor:\n",
            "In their article, Costa-Santos et al. [1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpreting\n",
            "reliability and agreement coefÔ¨Åcients arising from the same\n",
            "measurement situation. Debates and proposals about what\n",
            "the correct coefÔ¨Åcients to measure agreement and reliabilityare can be traced back to the early 1980s [2,3]. Various ap-\n",
            "proaches were discussed to overcome the ‚Äò‚Äòlimitations‚Äô‚Äô and‚Äò‚Äòdrawbacks‚Äô‚Äô of reliability measures (e.g., Refs. [4,5]), and\n",
            "even today, new alternatives are proposed (e.g., Refs. [6,7]). However, it seems that much of the confusion around re-\n",
            "liability and agreement estimation was and is caused by\n",
            "conceptual ambiguities. There are important differences\n",
            "between the concepts of agreement and reliability (e.g.,\n",
            "Refs. [8,9] ).Agreement points to the question, whether di-\n",
            "agnoses, scores, or judgments are identical or similar or thedegree to which they differ. In this situation, the absolutedegree of measurement error is of interest. Consequently,any variability between subjects or the distribution of therated trait in the population does not matter. For instance,\n",
            "percent agreement for nominal data or limits of agreement\n",
            "for interval and ratio data are excellent measures becausethey provide this very kind of information in a simple man-ner. On the other hand, there are the reliability coefÔ¨Åcients. Reliability is typically deÔ¨Åned as the ratio of variability be-tween scores of the same subjects (e.g., by different ratersor at different times) to the total variability of all scoresin the sample. Therefore, reliability coefÔ¨Åcients (e.g.,\n",
            "kappa, intraclass correlation coefÔ¨Åcient) provide informa-\n",
            "tion about the ability of the scores to distinguish betweensubjects. From this, it also follows that reliability coefÔ¨Å-cients must be low when there is little variability amongthe scores or diagnoses obtained from the instrument underinvestigation....\n",
            "\n",
            "üìù Generated Summary:\n",
            "--------------------------------------------------\n",
            "The difference between reliability and agreement\n",
            "\n",
            "üìä Compression Stats:\n",
            "üìè Original: 1997 characters\n",
            "üìè Summary: 48 characters\n",
            "üìà Compression ratio: 41.6:1\n",
            "‚úÖ Summarization function working correctly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ê Step 7: Summarize the Full Text\n",
        "\n",
        "Now let's try something interesting - what happens when we summarize the ENTIRE document in one go? This experiment will help us understand the limitations and challenges of working with large language models.\n",
        "\n",
        "## ü§î The Big Question: Context Window Limitations\n",
        "\n",
        "### **What's a Context Window?**\n",
        "A **context window** is the maximum amount of text an AI model can process at once. Think of it like the model's \"working memory\" - it can only \"see\" and \"remember\" a limited amount of text.\n",
        "\n",
        "### **FLAN-T5-Base Specifications:**\n",
        "- **üìè Maximum Input**: ~512 tokens (roughly 400-500 words)\n",
        "- **üß† Context Limit**: Cannot process documents longer than this limit\n",
        "- **‚ö†Ô∏è Truncation Risk**: Long texts get cut off, losing important information\n",
        "\n",
        "### **What Happens When Text is Too Long?**\n",
        "\n",
        "#### **üîÑ Automatic Truncation:**\n",
        "- Model automatically cuts off text after 512 tokens\n",
        "- **Lost Information**: Important content at the end gets ignored\n",
        "- **Incomplete Context**: Model only sees the beginning of the document\n",
        "- **Biased Summaries**: Results may not represent the full document\n",
        "\n",
        "#### **üìä Quality Implications:**\n",
        "- ‚úÖ **Works Well**: Short documents, single topics\n",
        "- ‚ö†Ô∏è **Problematic**: Long documents, multiple topics, important conclusions at the end\n",
        "- ‚ùå **Fails**: Very long documents where key points are distributed throughout\n",
        "\n",
        "### **Why This Experiment Matters:**\n",
        "1. **üß™ Educational**: See firsthand how context limits affect AI performance\n",
        "2. **üìà Comparison**: Compare single-shot vs. chunk-based approaches\n",
        "3. **üéØ Understanding**: Learn when each approach is appropriate\n",
        "4. **üîç Analysis**: Observe what information gets lost in truncation\n",
        "\n",
        "### **Real-World Implications:**\n",
        "- **üìö Academic Papers**: Often too long for single-pass summarization\n",
        "- **üìÑ Legal Documents**: Critical details might be at the end\n",
        "- **üì∞ News Articles**: Lead vs. conclusion information balance\n",
        "- **üìñ Books/Reports**: Require chunk-based processing for comprehensive summaries"
      ],
      "metadata": {
        "id": "k4HderO1uyiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Experiment: Summarize the entire document at once\n",
        "print(\"üß™ EXPERIMENT: Full Document Summarization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check the size of our full text\n",
        "print(f\"üìä Full document statistics:\")\n",
        "print(f\"üìè Total characters: {len(full_text):,}\")  # Hint: Our extracted text variable\n",
        "# Hint: Estimate tokens by counting words and multiplying by 1.3\n",
        "print(f\"üìù Estimated tokens: {len(full_text.split()) * 1.3:.0f}\")\n",
        "print(f\"‚ö†Ô∏è  Model limit: ~512 tokens\")\n",
        "print()\n",
        "\n",
        "# Analyze what will happen\n",
        "estimated_tokens = len(full_text.split()) * 1.3  # Hint: Same text variable as above\n",
        "if estimated_tokens > 500:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Document likely exceeds model's context window!\")\n",
        "    print(\"üìâ Expected behavior: Text will be truncated\")\n",
        "    print(\"üéØ This demonstrates the importance of chunking strategy\")\n",
        "else:\n",
        "    print(\"‚úÖ Document should fit within context window\")\n",
        "\n",
        "print(\"\\nüîÑ Generating full document summary...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Generate the summary\n",
        "# Hint: Use our summarize function with the full text\n",
        "abstractive_full_summary = summarize(full_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"üìñ FULL DOCUMENT SUMMARY:\")\n",
        "print(\"=\" * 40)\n",
        "print(abstractive_full_summary)  # Hint: Print our summary variable\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Analysis of the result\n",
        "print(f\"\\nüìä SUMMARY ANALYSIS:\")\n",
        "print(f\"üìè Summary length: {len(abstractive_full_summary)} characters\")  # Hint: Count summary characters\n",
        "print(f\"üìù Summary words: {len(abstractive_full_summary.split())} words\")  # Hint: Count summary words\n",
        "# Hint: Calculate compression by dividing original length by summary length\n",
        "print(f\"üìà Compression ratio: {len(full_text)/len(abstractive_full_summary):.1f}:1\")\n",
        "\n",
        "# Critical thinking questions\n",
        "print(f\"\\nü§î CRITICAL ANALYSIS:\")\n",
        "print(\"Questions to consider:\")\n",
        "print(\"‚Ä¢ Does this summary capture the main points from throughout the document?\")\n",
        "print(\"‚Ä¢ What information might be missing from the end of the document?\")\n",
        "print(\"‚Ä¢ How does this compare to what you'd expect from reading the full text?\")\n",
        "print(\"‚Ä¢ Would a chunk-based approach provide better coverage?\")\n",
        "\n",
        "# Preview what got truncated (if anything)\n",
        "if estimated_tokens > 500:\n",
        "    print(f\"\\n‚ö†Ô∏è  TRUNCATION ANALYSIS:\")\n",
        "    print(\"The model likely only processed the first ~400-500 words.\")\n",
        "    print(\"Here's roughly where the truncation occurred:\")\n",
        "    print(\"-\" * 30)\n",
        "    truncation_point = int(len(full_text) * 0.3)  # Rough estimate\n",
        "    print(f\"Last ~100 chars the model saw: ...{full_text[truncation_point-100:truncation_point]}\")\n",
        "    print(f\"First ~100 chars it missed: {full_text[truncation_point:truncation_point+100]}...\")\n"
      ],
      "metadata": {
        "id": "p3M8Mjc1uwji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4485fa-2958-47a9-b5cd-84c668b26065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ EXPERIMENT: Full Document Summarization\n",
            "============================================================\n",
            "üìä Full document statistics:\n",
            "üìè Total characters: 9,212\n",
            "üìù Estimated tokens: 1685\n",
            "‚ö†Ô∏è  Model limit: ~512 tokens\n",
            "\n",
            "‚ö†Ô∏è  WARNING: Document likely exceeds model's context window!\n",
            "üìâ Expected behavior: Text will be truncated\n",
            "üéØ This demonstrates the importance of chunking strategy\n",
            "\n",
            "üîÑ Generating full document summary...\n",
            "----------------------------------------\n",
            "üìñ FULL DOCUMENT SUMMARY:\n",
            "========================================\n",
            "The difference between reliability and agreement is a matter of conceptual ambiguities.\n",
            "========================================\n",
            "\n",
            "üìä SUMMARY ANALYSIS:\n",
            "üìè Summary length: 87 characters\n",
            "üìù Summary words: 12 words\n",
            "üìà Compression ratio: 105.9:1\n",
            "\n",
            "ü§î CRITICAL ANALYSIS:\n",
            "Questions to consider:\n",
            "‚Ä¢ Does this summary capture the main points from throughout the document?\n",
            "‚Ä¢ What information might be missing from the end of the document?\n",
            "‚Ä¢ How does this compare to what you'd expect from reading the full text?\n",
            "‚Ä¢ Would a chunk-based approach provide better coverage?\n",
            "\n",
            "‚ö†Ô∏è  TRUNCATION ANALYSIS:\n",
            "The model likely only processed the first ~400-500 words.\n",
            "Here's roughly where the truncation occurred:\n",
            "------------------------------\n",
            "Last ~100 chars the model saw: ... .to signify good agree-ment‚Äô‚Äô (page XX). In fact, Burdock et al. said the following:‚Äò‚ÄòA high intrac\n",
            "First ~100 chars it missed: lass correlation coefÔ¨Åcient, e.g. R/C210.75,\n",
            "means that there is relatively little residual variabil...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìè ROUGE Metric Comparison\n",
        "\n",
        "To evaluate how well our generated summaries capture key content, we'll use the **ROUGE metric** (Recall-Oriented Understudy for Gisting Evaluation). This is the gold standard for evaluating automatic summarization systems!\n",
        "\n",
        "## üéØ What is ROUGE?\n",
        "\n",
        "**ROUGE** compares the overlap between a generated summary and a reference summary using different text analysis methods:\n",
        "\n",
        "### **üìä ROUGE Variants:**\n",
        "\n",
        "#### **üî§ ROUGE-1: Single Word Overlap (Unigrams)**\n",
        "- **What it measures**: How many individual words appear in both summaries\n",
        "- **Good for**: Overall content coverage and vocabulary overlap\n",
        "- **Example**: \"The cat sat\" vs \"A cat was sitting\" ‚Üí shares \"cat\"\n",
        "\n",
        "#### **üîó ROUGE-2: Word Pair Overlap (Bigrams)**\n",
        "- **What it measures**: How many 2-word sequences appear in both summaries\n",
        "- **Good for**: Phrase-level similarity and word order preservation\n",
        "- **Example**: \"machine learning\" as a complete phrase vs. separate words\n",
        "\n",
        "#### **üìè ROUGE-L: Longest Common Subsequence**\n",
        "- **What it measures**: Longest sequence of words that appear in the same order\n",
        "- **Good for**: Structural similarity and sentence flow\n",
        "- **Example**: Preserves the logical flow of ideas between summaries\n",
        "\n",
        "### **üìà ROUGE Scores Explained:**\n",
        "\n",
        "#### **üéØ Precision**:\n",
        "- What percentage of words in the generated summary appear in the reference?\n",
        "- **High Precision** = Generated summary doesn't add irrelevant content\n",
        "\n",
        "#### **üîç Recall**:\n",
        "- What percentage of words in the reference appear in the generated summary?\n",
        "- **High Recall** = Generated summary captures most important content\n",
        "\n",
        "#### **‚öñÔ∏è F1-Score**:\n",
        "- Harmonic mean of precision and recall (balanced measure)\n",
        "- **High F1** = Good balance between completeness and relevance\n",
        "\n",
        "### **üé® Why Use ROUGE for Summarization?**\n",
        "- ‚úÖ **Objective Evaluation**: Provides quantitative quality measurements\n",
        "- ‚úÖ **Industry Standard**: Used in research and production systems\n",
        "- ‚úÖ **Multi-faceted**: Captures different aspects of summary quality\n",
        "- ‚úÖ **Comparative**: Allows comparison between different summarization approaches\n",
        "- ‚úÖ **Automated**: Can evaluate large numbers of summaries quickly\n",
        "\n",
        "### **‚ö†Ô∏è ROUGE Limitations:**\n",
        "- Requires reference summaries (human-written gold standards)\n",
        "- Focuses on word overlap, not semantic meaning\n",
        "- May not capture creative or paraphrased summaries well\n",
        "- Higher scores don't always mean better human-perceived quality"
      ],
      "metadata": {
        "id": "Rk6osScMiDXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Setup: Import and Define ROUGE Scoring Function\n",
        "\n",
        "We‚Äôll define a simple helper function to display precision, recall, and F1 for each ROUGE score.\n"
      ],
      "metadata": {
        "id": "Utpw9nrKTbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Setup: Import and Define ROUGE Scoring Function\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def compare_rouge(hypothesis, reference):\n",
        "    \"\"\"\n",
        "    Compare a generated summary (hypothesis) with a reference summary using ROUGE metrics.\n",
        "\n",
        "    Args:\n",
        "        hypothesis (str): The AI-generated summary to evaluate\n",
        "        reference (str): The human-written reference summary (ground truth)\n",
        "\n",
        "    Returns:\n",
        "        None: Prints formatted ROUGE scores\n",
        "    \"\"\"\n",
        "    # Initialize ROUGE scorer with all three metrics and stemming\n",
        "    scorer = rouge_scorer.RougeScorer(\n",
        "        ['rouge1', 'rouge2', 'rougeL'],\n",
        "        use_stemmer=True  # Reduces words to stems (e.g., \"running\" ‚Üí \"run\")\n",
        "    )\n",
        "\n",
        "    # Calculate scores by comparing reference to hypothesis\n",
        "    scores = scorer.score(reference, hypothesis)\n",
        "\n",
        "    # Display results in a clear, formatted way\n",
        "    print(\"üìä ROUGE EVALUATION RESULTS:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for metric_name, score_values in scores.items():\n",
        "        print(f\"\\nüîç {metric_name.upper()}:\")\n",
        "        print(f\"   üìà Precision: {score_values.precision:.4f}\")\n",
        "        print(f\"   üìâ Recall:    {score_values.recall:.4f}\")\n",
        "        print(f\"   ‚öñÔ∏è  F1-Score:  {score_values.fmeasure:.4f}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Provide interpretation guidance\n",
        "    print(\"\\nüí° INTERPRETATION GUIDE:\")\n",
        "    print(\"üìà Precision: How much of the generated content is relevant?\")\n",
        "    print(\"üìâ Recall: How much of the reference content was captured?\")\n",
        "    print(\"‚öñÔ∏è  F1-Score: Overall balanced quality measure\")\n",
        "    print(\"\\nüìä Score Ranges: 0.0 (worst) ‚Üí 1.0 (perfect match)\")\n",
        "\n",
        "print(\"‚úÖ ROUGE comparison function ready for use!\")\n"
      ],
      "metadata": {
        "id": "wFIVStjWjw25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5598593a-94c1-4483-8614-c768d8f08888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ROUGE comparison function ready for use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Reference Summary for Comparison\n",
        "\n",
        "Now we'll define a manually written summary to use as our **ground truth** and compare our AI-generated summaries against it. This reference summary represents what we consider to be a high-quality summary of the document.\n",
        "\n",
        "## üéØ Why We Need a Reference Summary:\n",
        "- **üìè Evaluation Standard**: Provides a benchmark for comparison\n",
        "- **üéì Quality Control**: Helps us assess how well our AI performs\n",
        "- **üìä Objective Metrics**: Enables quantitative evaluation using ROUGE\n",
        "- **üîç Analysis**: Allows us to identify strengths and weaknesses\n",
        "\n",
        "## üìù Creating Good Reference Summaries:\n",
        "- **Comprehensive**: Covers main points from throughout the document\n",
        "- **Concise**: Removes unnecessary details while preserving key information\n",
        "- **Accurate**: Faithfully represents the original content\n",
        "- **Well-written**: Uses clear, coherent language"
      ],
      "metadata": {
        "id": "lsiWH3pbkIGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù Define a manually written reference summary for comparison\n",
        "reference_summary = \"\"\"\n",
        "The letter highlights that agreement measures how similar or identical results are, focusing on absolute error,\n",
        "while reliability assesses how well an instrument distinguishes between subjects based on score variability.\n",
        "Confusion often arises from conceptual ambiguity between the two, despite decades of debate and evolving metrics.\"\"\"\n",
        "\n",
        "print(\"üìÑ REFERENCE SUMMARY DEFINED:\")\n",
        "print(\"=\" * 50)\n",
        "print(reference_summary)\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìè Reference length: {len(reference_summary)} characters\")\n",
        "print(f\"üìù Reference words: {len(reference_summary.split())} words\")\n",
        "print(\"\\n‚úÖ Reference summary ready for ROUGE comparison!\")"
      ],
      "metadata": {
        "id": "cFHUFaLjkHDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f15eea-4f23-4cc6-f1e3-7e20b6c41bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ REFERENCE SUMMARY DEFINED:\n",
            "==================================================\n",
            "\n",
            "The letter highlights that agreement measures how similar or identical results are, focusing on absolute error, \n",
            "while reliability assesses how well an instrument distinguishes between subjects based on score variability. \n",
            "Confusion often arises from conceptual ambiguity between the two, despite decades of debate and evolving metrics.\n",
            "==================================================\n",
            "üìè Reference length: 337 characters\n",
            "üìù Reference words: 46 words\n",
            "\n",
            "‚úÖ Reference summary ready for ROUGE comparison!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Apply ROUGE Comparison\n",
        "\n",
        "Now let's put our ROUGE evaluation to work! We'll compare our AI-generated full document summary against our manually written reference summary to see how well our model performed.\n",
        "\n",
        "## üîç What We're Testing:\n",
        "- **Quality Assessment**: How well does our AI capture the key points?\n",
        "- **Content Coverage**: Does it miss important information?\n",
        "- **Precision vs Recall**: Is it accurate vs comprehensive?\n",
        "- **Overall Performance**: Should we adjust our approach?"
      ],
      "metadata": {
        "id": "fmPNPUKpxUn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Compare our AI-generated summary with the reference summary\n",
        "print(\"üß™ EVALUATING OUR AI-GENERATED SUMMARY:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"üìñ Our AI-Generated Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(abstractive_full_summary)\n",
        "\n",
        "print(\"\\nüìÑ Reference Summary (Ground Truth):\")\n",
        "print(\"-\" * 40)\n",
        "print(reference_summary)\n",
        "\n",
        "# Apply ROUGE evaluation\n",
        "# Hint: Use our compare_rouge function with AI summary first, then reference\n",
        "compare_rouge(abstractive_full_summary, reference_summary)\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"\\nüìà SUMMARY COMPARISON:\")\n",
        "print(f\"üìè AI Summary length: {len(abstractive_full_summary)} characters\")\n",
        "print(f\"üìè Reference length: {len(reference_summary)} characters\")\n",
        "# Hint: Calculate ratio by dividing AI length by reference length\n",
        "print(f\"üìä Length ratio: {len(abstractive_full_summary)/len(reference_summary):.2f}:1\")\n",
        "\n",
        "# Interpretation help\n",
        "print(f\"\\nü§î ANALYSIS QUESTIONS:\")\n",
        "print(\"‚Ä¢ Which summary better captures the paper's main contributions?\")\n",
        "print(\"‚Ä¢ What key information might be missing from the AI summary?\")\n",
        "print(\"‚Ä¢ How do the ROUGE scores reflect the quality differences you observe?\")\n",
        "print(\"‚Ä¢ Would chunked summarization potentially perform better?\")"
      ],
      "metadata": {
        "id": "WSLXr6J9xY0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71db2680-0c4d-4a85-f5c9-dc139a83636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ EVALUATING OUR AI-GENERATED SUMMARY:\n",
            "============================================================\n",
            "üìñ Our AI-Generated Summary:\n",
            "----------------------------------------\n",
            "The difference between reliability and agreement is a matter of conceptual ambiguities.\n",
            "\n",
            "üìÑ Reference Summary (Ground Truth):\n",
            "----------------------------------------\n",
            "\n",
            "The letter highlights that agreement measures how similar or identical results are, focusing on absolute error, \n",
            "while reliability assesses how well an instrument distinguishes between subjects based on score variability. \n",
            "Confusion often arises from conceptual ambiguity between the two, despite decades of debate and evolving metrics.\n",
            "üìä ROUGE EVALUATION RESULTS:\n",
            "==================================================\n",
            "\n",
            "üîç ROUGE1:\n",
            "   üìà Precision: 0.6667\n",
            "   üìâ Recall:    0.1739\n",
            "   ‚öñÔ∏è  F1-Score:  0.2759\n",
            "\n",
            "üîç ROUGE2:\n",
            "   üìà Precision: 0.0909\n",
            "   üìâ Recall:    0.0222\n",
            "   ‚öñÔ∏è  F1-Score:  0.0357\n",
            "\n",
            "üîç ROUGEL:\n",
            "   üìà Precision: 0.3333\n",
            "   üìâ Recall:    0.0870\n",
            "   ‚öñÔ∏è  F1-Score:  0.1379\n",
            "==================================================\n",
            "\n",
            "üí° INTERPRETATION GUIDE:\n",
            "üìà Precision: How much of the generated content is relevant?\n",
            "üìâ Recall: How much of the reference content was captured?\n",
            "‚öñÔ∏è  F1-Score: Overall balanced quality measure\n",
            "\n",
            "üìä Score Ranges: 0.0 (worst) ‚Üí 1.0 (perfect match)\n",
            "\n",
            "üìà SUMMARY COMPARISON:\n",
            "üìè AI Summary length: 87 characters\n",
            "üìè Reference length: 337 characters\n",
            "üìä Length ratio: 0.26:1\n",
            "\n",
            "ü§î ANALYSIS QUESTIONS:\n",
            "‚Ä¢ Which summary better captures the paper's main contributions?\n",
            "‚Ä¢ What key information might be missing from the AI summary?\n",
            "‚Ä¢ How do the ROUGE scores reflect the quality differences you observe?\n",
            "‚Ä¢ Would chunked summarization potentially perform better?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîÑ Let's Try Different Summarization Methods\n",
        "\n",
        "Now that we've explored abstractive summarization with our full document, let's experiment with different approaches to see how they compare! Understanding various summarization techniques will help us choose the best method for different scenarios.\n",
        "\n",
        "## üéØ Why Compare Different Methods?\n",
        "\n",
        "### **üìä Method Comparison Benefits:**\n",
        "- **üîç Understanding Strengths**: Each method excels in different scenarios\n",
        "- **üìà Performance Analysis**: Compare quality, speed, and resource usage\n",
        "- **üé® Approach Diversity**: Abstractive vs. extractive vs. hybrid methods\n",
        "- **üõ†Ô∏è Tool Selection**: Learn when to use which technique\n",
        "\n",
        "### **üß™ What We'll Explore:**\n",
        "\n",
        "#### **üìù Extractive Summarization**\n",
        "- **How it works**: Selects and combines existing sentences from the original text\n",
        "- **Advantages**: Preserves original wording, factually accurate, faster processing\n",
        "- **Best for**: News articles, formal documents, when precision is critical\n",
        "- **Limitations**: Can sound choppy, may miss nuanced connections\n",
        "\n",
        "#### **üß† Abstractive Summarization** (What we just did)\n",
        "- **How it works**: Generates new text that captures the essence of the original\n",
        "- **Advantages**: More natural language, can rephrase and synthesize\n",
        "- **Best for**: Creative content, when readability is important\n",
        "- **Limitations**: May introduce errors, requires more computational power\n",
        "\n",
        "#### **‚ö° Hybrid Approaches**\n",
        "- **How it works**: Combines extractive and abstractive techniques\n",
        "- **Advantages**: Balances accuracy with readability\n",
        "- **Best for**: Complex documents, when you need both precision and flow"
      ],
      "metadata": {
        "id": "IVgVxI6tNFis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Extractive Summarization\n",
        "\n",
        "Let's implement a simple but effective extractive summarization approach that selects the most important sentences from our document.\n",
        "\n",
        "### üîç How Extractive Summarization Works\n",
        "\n",
        "Unlike abstractive summarization that generates new text, **extractive summarization** acts like a smart highlighter - it identifies and selects the most important sentences from the original document without changing them.\n",
        "\n",
        "#### **üßÆ The Algorithm Steps:**\n",
        "\n",
        "1. **üìù Sentence Tokenization**: Split the document into individual sentences\n",
        "2. **üîß Preprocessing**: Remove very short sentences (fragments) that don't contain meaningful information\n",
        "3. **üìä TF-IDF Scoring**: Calculate importance scores for each sentence\n",
        "4. **üìç Position Weighting**: Give higher importance to sentences appearing earlier in the document\n",
        "5. **üèÜ Selection**: Pick the top-scoring sentences while preserving their original order\n",
        "\n",
        "#### **üî¢ TF-IDF: The Heart of Sentence Scoring**\n",
        "\n",
        "**TF-IDF** (Term Frequency-Inverse Document Frequency) helps us identify which sentences contain the most important information:\n",
        "\n",
        "- **üìà Term Frequency (TF)**: How often important words appear in each sentence\n",
        "- **üìâ Inverse Document Frequency (IDF)**: How rare/unique those words are across all sentences\n",
        "- **üéØ Combined Score**: Sentences with frequent important words AND rare key terms score highest\n",
        "\n",
        "#### **üìç Why Position Matters**\n",
        "\n",
        "Research shows that in most documents (especially academic papers and news articles), the most important information often appears early. Our algorithm applies **position weighting**:\n",
        "\n",
        "- **First sentences**: Get full weight (1.0)\n",
        "- **Later sentences**: Get progressively lower weights (down to 0.5)\n",
        "- **Result**: Early sentences with good content beat later sentences with similar content\n",
        "\n",
        "#### **‚öñÔ∏è Final Sentence Selection**\n",
        "\n",
        "The algorithm combines:\n",
        "- **Content importance** (TF-IDF scores)\n",
        "- **Position importance** (earlier = better)\n",
        "- **Quality filtering** (removes sentence fragments)\n",
        "\n",
        "Then selects the top N sentences while **preserving their original order** for natural reading flow.\n",
        "\n",
        "### üÜö Extractive vs. Abstractive Comparison\n",
        "\n",
        "| Aspect | Extractive | Abstractive |\n",
        "|--------|------------|-------------|\n",
        "| **Accuracy** | ‚úÖ High (uses original words) | ‚ö†Ô∏è Can introduce errors |\n",
        "| **Fluency** | ‚ö†Ô∏è May sound choppy | ‚úÖ Natural, flowing text |\n",
        "| **Speed** | ‚úÖ Fast processing | ‚ö†Ô∏è Slower, more complex |\n",
        "| **Creativity** | ‚ùå Cannot rephrase | ‚úÖ Can synthesize ideas |\n",
        "| **Factual Safety** | ‚úÖ Preserves exact wording | ‚ö†Ô∏è May alter meanings |"
      ],
      "metadata": {
        "id": "H7Jce69OKeB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def extractive_summarization(text, num_sentences=5):\n",
        "    \"\"\"\n",
        "    Extract the most important sentences from text using TF-IDF scoring\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    # Hint: Use nltk.sent_tokenize() to split text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Consider adding sentence length filtering\n",
        "    # Hint: Keep sentences with more than 5 words using len(s.split()) > 5\n",
        "    sentences = [s for s in sentences if len(s.split()) > 5]  # Remove very short sentences\n",
        "\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    # Hint: Use TfidfVectorizer with stop_words='english' and lowercase=True\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "\n",
        "    # Fit and transform sentences\n",
        "    try:\n",
        "        # Hint: Use vectorizer.fit_transform() to create the TF-IDF matrix\n",
        "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "        # Calculate sentence scores (sum of TF-IDF scores)\n",
        "        # Hint: Use np.array() and .sum(axis=1) to sum TF-IDF scores for each sentence\n",
        "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "        # Optional: Add position weighting (earlier sentences often more important)\n",
        "        # Hint: Use np.linspace() to create weights from 1.0 to 0.5\n",
        "        position_weights = np.linspace(1.0, 0.5, len(sentences))\n",
        "        sentence_scores = sentence_scores * position_weights\n",
        "\n",
        "        # Get top sentences\n",
        "        # Hint: Use .argsort() to get indices, then [-num_sentences:] for top scores\n",
        "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
        "        top_indices = sorted(top_indices)  # Keep original order\n",
        "\n",
        "        # Extract top sentences\n",
        "        summary_sentences = [sentences[i] for i in top_indices]\n",
        "        return \" \".join(summary_sentences)\n",
        "\n",
        "    except ValueError:\n",
        "        # Fallback: return first few sentences if TF-IDF fails\n",
        "        return \" \".join(sentences[:num_sentences])\n",
        "\n",
        "# Generate extractive summary\n",
        "# Hint: Call our function with full_text and num_sentences=5\n",
        "full_extractive_summary = extractive_summarization(full_text, num_sentences=5)\n"
      ],
      "metadata": {
        "id": "-LBn6qgsKh7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ View Our Extractive Summary\n",
        "\n",
        "Now let's see what our extractive summarization algorithm produced! This summary consists of the 5 most important sentences selected from the original document, preserving their exact wording and original order.\n",
        "\n",
        "## üîç What to Look For:\n",
        "\n",
        "### **üìä Content Analysis:**\n",
        "- **Key Topics**: Does it capture the main themes from the document?\n",
        "- **Important Details**: Are critical facts and findings included?\n",
        "- **Completeness**: Does it feel like a comprehensive overview?\n",
        "\n",
        "### **üìù Quality Assessment:**\n",
        "- **Coherence**: Do the selected sentences flow well together?\n",
        "- **Factual Accuracy**: All content is guaranteed to be from the original (no hallucinations!)\n",
        "- **Readability**: Is it easy to understand and follow?\n",
        "\n",
        "### **üÜö Comparison Points:**\n",
        "- **vs. Abstractive**: How does this compare to our AI-generated summary?\n",
        "- **vs. Reference**: How well does it match our manual reference summary?\n",
        "- **Content Coverage**: What information is preserved vs. lost?\n",
        "\n",
        "## üéØ Remember:\n",
        "Extractive summaries excel at **preserving exact wording** and **maintaining factual accuracy**, but may sometimes feel less fluid than abstractive summaries that can rephrase and connect ideas more naturally."
      ],
      "metadata": {
        "id": "0Ll2ldBP1rRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the Extractive summary text\n",
        "print(full_extractive_summary)"
      ],
      "metadata": {
        "id": "casWEol5K6iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ff704e-d9c4-4e1c-c0ae-37e40eb171e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]provide a valuable\n",
            "example about the difÔ¨Åculties in comparing and interpreting\n",
            "reliability and agreement coefÔ¨Åcients arising from the same\n",
            "measurement situation. For instance,\n",
            "percent agreement for nominal data or limits of agreement\n",
            "for interval and ratio data are excellent measures becausethey provide this very kind of information in a simple man-ner. This occurs when the range of obtainedscores is restricted or prevalence is very high or very low.For example, if all raters rate medical students as ‚Äò‚Äòexcel-\n",
            "lent,‚Äô‚Äô the agreement is perfect, but the reliability of the\n",
            "scale is zero because there is no between-subject variance.It should also be noted that exact agreement among ratersor over time does not enter into (most of) the formulasfor reliability because all that matters is that the subjectsare rank ordered similarly across time or by different raters. E-mail address: jan.kottner@email.de\n",
            "David L. Streiner\n",
            "Department of Psychiatry\n",
            "University of Toronto, Toronto\n",
            "Ontario, Canada\n",
            "References\n",
            "[1] Costa-Santos C, Bernardes J, Ayres-de-Campos D, Costa A, Costa C.\n",
            "The limits of agreement and the intraclass correlation coefÔ¨Åcient may\n",
            "be inconsistent in the interpretation of agreement. doi: 10.1016/j.jclinepi.2010.12.001\n",
            "Observer reliability and agreement: differences,\n",
            "difÔ¨Åculties, and controversies\n",
            "In Reply:\n",
            "We thank Kottner and Streiner for their interest and use-\n",
            "ful comments on our article ‚Äò‚ÄòThe limits of agreement and\n",
            "the intraclass correlation coefÔ¨Åcient may be inconsistent inthe interpretation of agreement,‚Äô‚Äô in Journal of Clinical\n",
            "Epidemiology , 2010 [1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets check the rouge scores\n",
        "\n",
        "compare_rouge(full_extractive_summary, reference_summary)"
      ],
      "metadata": {
        "id": "FtmOirSPPjG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcd3710-cd19-4a19-e3d7-8ac96a9390de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä ROUGE EVALUATION RESULTS:\n",
            "==================================================\n",
            "\n",
            "üîç ROUGE1:\n",
            "   üìà Precision: 0.0600\n",
            "   üìâ Recall:    0.3261\n",
            "   ‚öñÔ∏è  F1-Score:  0.1014\n",
            "\n",
            "üîç ROUGE2:\n",
            "   üìà Precision: 0.0080\n",
            "   üìâ Recall:    0.0444\n",
            "   ‚öñÔ∏è  F1-Score:  0.0136\n",
            "\n",
            "üîç ROUGEL:\n",
            "   üìà Precision: 0.0480\n",
            "   üìâ Recall:    0.2609\n",
            "   ‚öñÔ∏è  F1-Score:  0.0811\n",
            "==================================================\n",
            "\n",
            "üí° INTERPRETATION GUIDE:\n",
            "üìà Precision: How much of the generated content is relevant?\n",
            "üìâ Recall: How much of the reference content was captured?\n",
            "‚öñÔ∏è  F1-Score: Overall balanced quality measure\n",
            "\n",
            "üìä Score Ranges: 0.0 (worst) ‚Üí 1.0 (perfect match)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ Multi-Stage (Hierarchical) Summarization\n",
        "\n",
        "Think of this like a **Champions League tournament** for text summarization! We'll summarize our text in rounds until we get one final champion summary.\n",
        "\n",
        "## üéØ Why Tournament Style?\n",
        "\n",
        "- **üìè Context Problem**: Our model only handles ~500 tokens at once\n",
        "- **üìö Large Documents**: We might have 5000+ tokens across chunks\n",
        "- **üîÑ Solution**: Summarize in rounds like a sports tournament!\n",
        "\n",
        "## üèüÔ∏è How It Works:\n",
        "\n",
        "1. **üîµ Round 1**: Summarize each chunk ‚Üí Get smaller summaries\n",
        "2. **üîç Check**: Do all summaries fit in context window?\n",
        "3. **üîÑ Round 2+**: If too big, summarize the summaries again\n",
        "4. **üèÜ Victory**: Repeat until one final summary fits"
      ],
      "metadata": {
        "id": "_5NdjY6dOWMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize Resumes (CV)"
      ],
      "metadata": {
        "id": "RElxXGYyr85W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDG9iDDZr7u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-XHXIqDr7z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üèÜ Multi-Stage Summarization: Tournament Style!\n",
        "\n",
        "def hierarchical_summarization(chunks, max_context_tokens=500, target_summary_tokens=100):\n",
        "    \"\"\"\n",
        "    Summarize text in tournament-style rounds until we get a final summary.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of text chunks to summarize\n",
        "        max_context_tokens (int): Maximum tokens our model can handle\n",
        "        target_summary_tokens (int): Target length for each summary\n",
        "\n",
        "    Returns:\n",
        "        str: Final hierarchical summary\n",
        "    \"\"\"\n",
        "\n",
        "    def estimate_tokens(text):\n",
        "        \"\"\"Simple token estimation: ~1.3 tokens per word\"\"\"\n",
        "        return int(len(text.split()) * 1.3)\n",
        "\n",
        "    def can_fit_in_context(summaries, max_tokens):\n",
        "        \"\"\"Check if all summaries together fit in context window\"\"\"\n",
        "        total_text = \"\\n\".join(summaries)\n",
        "        return estimate_tokens(total_text) <= max_tokens\n",
        "\n",
        "    # üèüÔ∏è Tournament Setup\n",
        "    print(\"üèÜ STARTING SUMMARIZATION TOURNAMENT!\")\n",
        "    print(\"‚ïê\" * 50)\n",
        "\n",
        "    current_round = 1\n",
        "    current_texts = chunks.copy()\n",
        "\n",
        "    # üîÑ Tournament Loop: Keep playing until we have a champion!\n",
        "    while len(current_texts) > 1:\n",
        "        print(f\"\\nüîµ ROUND {current_round}: Processing {len(current_texts)} texts\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        round_summaries = []\n",
        "\n",
        "        # üéØ If we have too many texts, group them before summarizing\n",
        "        if len(current_texts) > 20:\n",
        "            print(f\"   üì¶ Grouping {len(current_texts)} texts into batches for efficiency...\")\n",
        "            # Group texts into batches that fit in context window\n",
        "            batch_size = 5  # Process 5 texts at a time\n",
        "            for i in range(0, len(current_texts), batch_size):\n",
        "                batch = current_texts[i:i+batch_size]\n",
        "                batch_text = \"\\n\".join(batch)\n",
        "\n",
        "                print(f\"   ‚öΩ Batch {i//batch_size + 1}: Summarizing {len(batch)} texts ({estimate_tokens(batch_text)} tokens)...\")\n",
        "\n",
        "                if estimate_tokens(batch_text) <= max_context_tokens:\n",
        "                    summary = summarize(batch_text)\n",
        "                    round_summaries.append(summary)\n",
        "                    print(f\"   ‚úÖ Result: {estimate_tokens(summary)} tokens\")\n",
        "                else:\n",
        "                    # If batch is still too big, process individually\n",
        "                    for j, text in enumerate(batch):\n",
        "                        summary = summarize(text)\n",
        "                        round_summaries.append(summary)\n",
        "        else:\n",
        "            # üìä Regular processing for smaller numbers\n",
        "            for i, text in enumerate(current_texts, 1):\n",
        "                print(f\"   ‚öΩ Match {i}: Summarizing text {i} ({estimate_tokens(text)} tokens)...\")\n",
        "\n",
        "                summary = summarize(text)\n",
        "                round_summaries.append(summary)\n",
        "\n",
        "                print(f\"   ‚úÖ Result: {estimate_tokens(summary)} tokens\")\n",
        "\n",
        "        # üîç Check tournament status\n",
        "        if len(round_summaries) == 1:\n",
        "            # üèÜ We have our champion!\n",
        "            print(f\"\\nüèÜ TOURNAMENT COMPLETE!\")\n",
        "            print(f\"üéâ Champion Summary: {estimate_tokens(round_summaries[0])} tokens\")\n",
        "            return round_summaries[0]\n",
        "        elif can_fit_in_context(round_summaries, max_context_tokens):\n",
        "            # üîÑ Final round: combine all summaries\n",
        "            print(f\"\\nüèÅ FINAL ROUND: Combining {len(round_summaries)} summaries\")\n",
        "            combined_text = \"\\n\".join(round_summaries)\n",
        "            print(f\"   üìä Combined length: {estimate_tokens(combined_text)} tokens\")\n",
        "\n",
        "            final_summary = summarize(combined_text)\n",
        "            print(f\"\\nüèÜ TOURNAMENT COMPLETE!\")\n",
        "            print(f\"üéâ Champion Summary: {estimate_tokens(final_summary)} tokens\")\n",
        "            return final_summary\n",
        "        else:\n",
        "            # üîÑ Need another round\n",
        "            total_tokens = sum(estimate_tokens(s) for s in round_summaries)\n",
        "            print(f\"   ‚ö†Ô∏è  Combined summaries: {total_tokens} tokens (limit: {max_context_tokens})\")\n",
        "            print(f\"   üîÑ Advancing to next round with {len(round_summaries)} texts...\")\n",
        "            current_texts = round_summaries\n",
        "            current_round += 1\n",
        "\n",
        "            # Safety check to prevent infinite loops\n",
        "            if current_round > 10:\n",
        "                print(\"‚ö†Ô∏è  Maximum rounds reached, returning best available summary\")\n",
        "                return \"\\n\".join(round_summaries[:3])  # Return first 3 summaries\n",
        "\n",
        "    # üèÜ Single text case\n",
        "    return current_texts[0]\n",
        "\n",
        "# üéÆ Start the Tournament!\n",
        "print(\"üéÆ LAUNCHING HIERARCHICAL SUMMARIZATION TOURNAMENT!\")\n",
        "print(\"‚öΩ Let's see how many rounds our document needs...\\n\")\n",
        "\n",
        "# üìä First, let's check our chunks and create appropriate sized chunks for the tournament\n",
        "print(\"üìä PREPARING FOR TOURNAMENT:\")\n",
        "print(f\"üìö Current chunks: {len(chunks)}\")\n",
        "print(f\"üìè Average chunk size: {sum(len(chunk) for chunk in chunks) // len(chunks)} characters\")\n",
        "\n",
        "# üîß Create smaller chunks if needed (aiming for ~400 characters ‚âà 300 tokens)\n",
        "tournament_chunks = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    estimated_tokens = int(len(chunk.split()) * 1.3)\n",
        "    if estimated_tokens > 400:  # If chunk is too big, split it further\n",
        "        # Split large chunk into smaller pieces\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        small_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(small_chunk) + len(sentence) <= 400:\n",
        "                small_chunk += sentence + \" \"\n",
        "            else:\n",
        "                if small_chunk:\n",
        "                    tournament_chunks.append(small_chunk.strip())\n",
        "                small_chunk = sentence + \" \"\n",
        "        if small_chunk:\n",
        "            tournament_chunks.append(small_chunk.strip())\n",
        "    else:\n",
        "        tournament_chunks.append(chunk)\n",
        "\n",
        "print(f\"üèüÔ∏è Tournament-ready chunks: {len(tournament_chunks)}\")\n",
        "for i, chunk in enumerate(tournament_chunks[:3]):  # Show first 3\n",
        "    tokens = int(len(chunk.split()) * 1.3)\n",
        "    print(f\"   Chunk {i+1}: {tokens} estimated tokens\")\n",
        "\n",
        "hierarchical_summary = hierarchical_summarization(tournament_chunks, max_context_tokens=500, target_summary_tokens=100)\n",
        "\n",
        "# üìä Tournament Results\n",
        "print(\"\\n\" + \"‚ïê\" * 60)\n",
        "print(\"üìä FINAL TOURNAMENT RESULTS:\")\n",
        "print(\"‚ïê\" * 60)\n",
        "print(hierarchical_summary)\n",
        "print(\"‚ïê\" * 60)\n",
        "\n",
        "# üèÜ Victory Statistics\n",
        "print(f\"\\nüèÜ VICTORY STATISTICS:\")\n",
        "print(f\"üìè Final summary length: {len(hierarchical_summary)} characters\")\n",
        "print(f\"üéØ Estimated tokens: {int(len(hierarchical_summary.split()) * 1.3)}\")\n",
        "print(f\"üìö Original chunks processed: {len(chunks)}\")\n",
        "print(f\"‚úÖ Tournament summarization complete!\")"
      ],
      "metadata": {
        "id": "T55Pr7fYNS9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bf6a2f-e541-4d1d-f0b6-0cc0ceb5e87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéÆ LAUNCHING HIERARCHICAL SUMMARIZATION TOURNAMENT!\n",
            "‚öΩ Let's see how many rounds our document needs...\n",
            "\n",
            "üìä PREPARING FOR TOURNAMENT:\n",
            "üìö Current chunks: 5\n",
            "üìè Average chunk size: 1841 characters\n",
            "üèüÔ∏è Tournament-ready chunks: 5\n",
            "   Chunk 1: 371 estimated tokens\n",
            "   Chunk 2: 347 estimated tokens\n",
            "   Chunk 3: 327 estimated tokens\n",
            "üèÜ STARTING SUMMARIZATION TOURNAMENT!\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üîµ ROUND 1: Processing 5 texts\n",
            "----------------------------------------\n",
            "   ‚öΩ Match 1: Summarizing text 1 (371 tokens)...\n",
            "   ‚úÖ Result: 7 tokens\n",
            "   ‚öΩ Match 2: Summarizing text 2 (347 tokens)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ Result: 14 tokens\n",
            "   ‚öΩ Match 3: Summarizing text 3 (327 tokens)...\n",
            "   ‚úÖ Result: 1 tokens\n",
            "   ‚öΩ Match 4: Summarizing text 4 (321 tokens)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ Result: 93 tokens\n",
            "   ‚öΩ Match 5: Summarizing text 5 (317 tokens)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ Result: 32 tokens\n",
            "\n",
            "üèÅ FINAL ROUND: Combining 5 summaries\n",
            "   üìä Combined length: 149 tokens\n",
            "\n",
            "üèÜ TOURNAMENT COMPLETE!\n",
            "üéâ Champion Summary: 7 tokens\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "üìä FINAL TOURNAMENT RESULTS:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "The difference between reliability and agreement.\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üèÜ VICTORY STATISTICS:\n",
            "üìè Final summary length: 49 characters\n",
            "üéØ Estimated tokens: 7\n",
            "üìö Original chunks processed: 5\n",
            "‚úÖ Tournament summarization complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now assess the Rouge results\n",
        "compare_rouge(hierarchical_summary, reference_summary)"
      ],
      "metadata": {
        "id": "Saxe_so1thkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cdb574-b7dd-4b63-db69-7dc3df2700ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä ROUGE EVALUATION RESULTS:\n",
            "==================================================\n",
            "\n",
            "üîç ROUGE1:\n",
            "   üìà Precision: 0.8333\n",
            "   üìâ Recall:    0.1087\n",
            "   ‚öñÔ∏è  F1-Score:  0.1923\n",
            "\n",
            "üîç ROUGE2:\n",
            "   üìà Precision: 0.0000\n",
            "   üìâ Recall:    0.0000\n",
            "   ‚öñÔ∏è  F1-Score:  0.0000\n",
            "\n",
            "üîç ROUGEL:\n",
            "   üìà Precision: 0.5000\n",
            "   üìâ Recall:    0.0652\n",
            "   ‚öñÔ∏è  F1-Score:  0.1154\n",
            "==================================================\n",
            "\n",
            "üí° INTERPRETATION GUIDE:\n",
            "üìà Precision: How much of the generated content is relevant?\n",
            "üìâ Recall: How much of the reference content was captured?\n",
            "‚öñÔ∏è  F1-Score: Overall balanced quality measure\n",
            "\n",
            "üìä Score Ranges: 0.0 (worst) ‚Üí 1.0 (perfect match)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Challenge Tracks: Take Your Summarization to the Next Level!\n",
        "\n",
        "Congratulations! You've built a complete document summarization system. Now it's time to push the boundaries and explore advanced techniques. Choose one or more tracks below to enhance your skills and improve results.\n",
        "\n",
        "## üéØ Track 1: Multi-Model Comparison Arena\n",
        "**Description**: Test different language models to find the best summarizer for your use case.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How different models handle the same content\n",
        "- Performance vs. quality trade-offs\n",
        "- Model selection strategies for production systems\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Model Selection**: Choose 2-3 models (T5-small, BART, Pegasus, or GPT-based models)\n",
        "2. **Standardized Testing**: Run the same chunks through each model\n",
        "3. **ROUGE Comparison**: Evaluate all models against your reference summary\n",
        "4. **Speed Benchmarking**: Time each model's processing speed\n",
        "5. **Quality Analysis**: Compare output readability and accuracy\n",
        "6. **Recommendation**: Document which model works best for which scenarios\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 2: Hybrid Extractive-Abstractive Pipeline\n",
        "**Description**: Combine the best of both worlds - use extractive summarization to select important content, then abstractive to make it flow naturally.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How to chain different summarization approaches\n",
        "- When extraction vs. abstraction is more appropriate\n",
        "- Pipeline optimization techniques\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Stage 1**: Use extractive summarization to identify top sentences\n",
        "2. **Content Filtering**: Remove redundant or low-quality extractions\n",
        "3. **Stage 2**: Apply abstractive summarization to extracted content\n",
        "4. **Quality Control**: Compare hybrid results vs. pure approaches\n",
        "5. **Parameter Tuning**: Experiment with extraction ratios (how much to extract before abstracting)\n",
        "6. **Evaluation**: Test hybrid approach against both pure methods using ROUGE\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 3: Domain-Specific Optimization\n",
        "**Description**: Customize your summarization system for specific document types (academic papers, news, legal documents, etc.).\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How document structure affects summarization quality\n",
        "- Domain-specific prompt engineering\n",
        "- Specialized evaluation metrics\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Document Analysis**: Identify unique features of your chosen domain\n",
        "2. **Custom Chunking**: Adapt chunking strategy for document structure (abstracts, conclusions, etc.)\n",
        "3. **Specialized Prompts**: Create domain-specific prompts for your model\n",
        "4. **Position Weighting**: Adjust importance of different document sections\n",
        "5. **Domain Metrics**: Develop evaluation criteria beyond ROUGE (factual accuracy, terminology preservation)\n",
        "6. **Validation**: Test with multiple documents from your chosen domain\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 4: Intelligent Chunk Clustering for Better Summarization\n",
        "**Description**: Use machine learning to group similar chunks together before summarization, ensuring your final summary covers all major topics without redundancy.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How to convert text into numerical embeddings\n",
        "- Unsupervised learning with clustering algorithms\n",
        "- Topic modeling and content organization\n",
        "- How to balance topic coverage in summaries\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Generate Embeddings**: Use SentenceTransformers to convert each chunk into vector embeddings\n",
        "2. **Apply Clustering**: Use K-means or DBSCAN to group semantically similar chunks\n",
        "3. **Analyze Clusters**: Visualize clusters and identify what topics each represents\n",
        "4. **Smart Selection**: Choose representative chunks from each cluster for summarization\n",
        "5. **Topic-Balanced Summary**: Ensure final summary covers all major topic clusters\n",
        "6. **Evaluation**: Compare cluster-based vs. sequential summarization for topic diversity\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 5: Advanced Evaluation & Quality Metrics\n",
        "**Description**: Go beyond ROUGE to develop comprehensive quality assessment using multiple evaluation approaches.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- Limitations of current evaluation metrics\n",
        "- Multi-dimensional quality assessment\n",
        "- How to build robust evaluation pipelines\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Semantic Similarity**: Implement sentence embedding-based similarity (using models like SentenceTransformers)\n",
        "2. **Factual Accuracy**: Develop methods to check if key facts are preserved\n",
        "3. **Readability Analysis**: Measure text complexity and flow (using libraries like textstat)\n",
        "4. **Coverage Analysis**: Ensure summaries represent content from throughout the document\n",
        "5. **Human Evaluation**: Design surveys to collect human quality ratings\n",
        "6. **Comprehensive Dashboard**: Create visualization showing all quality dimensions\n",
        "7. **Quality Predictor**: Build a model that predicts summary quality without reference summaries\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Getting Started Tips:\n",
        "\n",
        "### üìö **Research Phase** (for all tracks):\n",
        "- Read recent papers on your chosen topic\n",
        "- Look for existing implementations on GitHub\n",
        "- Check Hugging Face model hub for relevant models\n",
        "\n",
        "### üõ†Ô∏è **Implementation Phase**:\n",
        "- Start with small experiments before building the full system\n",
        "- Document your findings and compare results systematically\n",
        "- Use version control to track different approaches\n",
        "\n",
        "### üìä **Evaluation Phase**:\n",
        "- Always compare against your baseline system\n",
        "- Use multiple documents for testing\n",
        "- Consider both quantitative metrics and qualitative analysis\n",
        "\n",
        "### üöÄ **Presentation Phase**:\n",
        "- Document your methodology clearly\n",
        "- Include visualizations of your results\n",
        "- Discuss limitations and future improvements\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Pro Tips:\n",
        "- **Start Simple**: Pick one track and master it before combining approaches\n",
        "- **Document Everything**: Keep detailed notes on what works and what doesn't\n",
        "- **Share Results**: Consider writing a blog post or creating a demo\n",
        "- **Think Production**: How would you deploy this in a real-world system?\n",
        "\n",
        "Choose your adventure and push the boundaries of text summarization! üöÄ"
      ],
      "metadata": {
        "id": "uB7BOIqOGVg4"
      }
    }
  ]
}